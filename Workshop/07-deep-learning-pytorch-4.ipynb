{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "144557d9-109a-4a6a-9143-86edebb449f0",
      "metadata": {
        "id": "144557d9-109a-4a6a-9143-86edebb449f0"
      },
      "source": [
        "# Deep Learning with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this workshop, we will try to build some feedforward models to do sentiment analysis, using pytorch, a deep learning library: https://pytorch.org/\n"
      ],
      "metadata": {
        "id": "s0VE6IDqznTv"
      },
      "id": "s0VE6IDqznTv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup GPU environment\n",
        "\n",
        "To use a free GPU in Google Colab, go to \"Runtime\" > \"Change runtime type\" and select \"GPU\" as the hardware accelerator.\n",
        "\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "- Accessing Colab: Open Google Colab in your browser and sign in with your Google account.\n",
        "\n",
        "- Creating a Notebook: Create a new notebook by clicking on \"New Notebook\".\n",
        "Enabling GPU:\n",
        "\n",
        "- Go to the \"Runtime\" menu.\n",
        "\n",
        "- Select \"Change runtime type\".\n",
        "\n",
        "- In the pop-up window, choose \"GPU\" as the hardware accelerator. Click \"Save\".\n",
        "\n",
        "Now you will need pandas, torch to run this code (pip install pandas torch)."
      ],
      "metadata": {
        "id": "YwWrlwK_zH1_"
      },
      "id": "YwWrlwK_zH1_"
    },
    {
      "cell_type": "markdown",
      "id": "9dc18c86-b744-4785-9141-121287129aad",
      "metadata": {
        "id": "9dc18c86-b744-4785-9141-121287129aad"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47b210e2-662b-47c2-8c95-60b8fbd5529a",
      "metadata": {
        "id": "47b210e2-662b-47c2-8c95-60b8fbd5529a"
      },
      "outputs": [],
      "source": [
        "!pip install pandas scikit-learn torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can run the below code to verify that GPU has been enabled:"
      ],
      "metadata": {
        "id": "NLC8GS22zyjy"
      },
      "id": "NLC8GS22zyjy"
    },
    {
      "cell_type": "code",
      "source": [
        "# imports are always needed\n",
        "import torch\n",
        "\n",
        "\n",
        "# get index of currently selected device\n",
        "print(f\"current device: {torch.cuda.current_device()}\") # returns 0 in my case\n",
        "\n",
        "\n",
        "# get number of GPUs available\n",
        "print(f\"number of GPU available: {torch.cuda.device_count()}\") # returns 1 in my case\n",
        "\n",
        "\n",
        "# get the name of the device\n",
        "print(f\"name of the device: {torch.cuda.get_device_name(0)}\") # good old Tesla K80\n",
        "\n",
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgDk7aOfz-iK",
        "outputId": "f679972e-941b-42ae-e6be-1f96bf69b23a"
      },
      "id": "pgDk7aOfz-iK",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current device: 0\n",
            "number of GPU available: 1\n",
            "name of the device: Tesla T4\n",
            "Using device: cuda\n",
            "\n",
            "Tesla T4\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-6d3d122c973b>:27: FutureWarning: `torch.cuda.memory_cached` has been renamed to `torch.cuda.memory_reserved`\n",
            "  print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af81e308-447f-441e-a15e-24dc3505d347",
      "metadata": {
        "id": "af81e308-447f-441e-a15e-24dc3505d347"
      },
      "source": [
        "## Loading dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acb05780-9a04-48c6-a160-a19359b40b45",
      "metadata": {
        "id": "acb05780-9a04-48c6-a160-a19359b40b45"
      },
      "source": [
        "First let's prepare the data. We are using 1000 yelp reviews, nnotated with either positive or negative sentiments. You can download the dataset file from canvas, and upload it to the current working directory in the Google Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fabcb7e7-3deb-4236-845d-fc6a833858cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fabcb7e7-3deb-4236-845d-fc6a833858cf",
        "outputId": "68f26080-1127-4010-c4cb-5574e4b90dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences = 1000\n",
            "\n",
            "Data:\n",
            "                                    sentence  label\n",
            "0                   Wow... Loved this place.      1\n",
            "1                         Crust is not good.      0\n",
            "2  Not tasty and the texture was just nasty.      0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "corpus = \"07-yelp-dataset.txt\"\n",
        "df = pd.read_csv(corpus, names=['sentence', 'label'], sep='\\t')\n",
        "print(\"Number of sentences =\", len(df))\n",
        "print(\"\\nData:\")\n",
        "print(df.iloc[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb12901d-daec-4a76-972a-5682efa29725",
      "metadata": {
        "id": "eb12901d-daec-4a76-972a-5682efa29725"
      },
      "source": [
        "Next, let's create the train/dev/test partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "83b79035-b049-40e8-bfc2-dbf080ff9c4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83b79035-b049-40e8-bfc2-dbf080ff9c4a",
        "outputId": "e17c8dbe-c12a-4dd9-dee2-d59e51065dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Wow... Loved this place.\n",
            "0 I'm super pissd.\n",
            "0 Spend your money elsewhere.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "sentences = df['sentence'].values\n",
        "labels = df['label'].values\n",
        "\n",
        "#partition data into 80/10/10 for train/dev/test\n",
        "sentences_train, y_train = sentences[:800], labels[:800]\n",
        "sentences_dev, y_dev = sentences[800:900], labels[800:900]\n",
        "sentences_test, y_test = sentences[900:1000], labels[900:1000]\n",
        "\n",
        "#convert label list into arrays\n",
        "y_train = np.array(y_train)\n",
        "y_dev = np.array(y_dev)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(y_train[0], sentences_train[0])\n",
        "print(y_dev[0], sentences_dev[0])\n",
        "print(y_test[0], sentences_test[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20f556a3-1e50-487e-8026-186ec505725a",
      "metadata": {
        "id": "20f556a3-1e50-487e-8026-186ec505725a"
      },
      "source": [
        "## Building vocabulary set and vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9a2d09-640f-493d-8058-5cf4f97e3df2",
      "metadata": {
        "id": "3b9a2d09-640f-493d-8058-5cf4f97e3df2"
      },
      "source": [
        "In this workshop, we will employ the `tokenizer` function provided by PyTorch to process our data. After tokenization, the next step involves using `build_vocab_from_iterator` to construct a frequency dictionary for our vocabulary. Moreover, we incorporate two special tokens, `<unk>` and `<pad>`, into our vocabulary set. The `<unk>` token is designated for managing tokens that have not been seen during training, ensuring the model can handle new or rare words. The `<pad>` token, on the other hand, will be utilized later to pad sequences of varying lengths, allowing us to standardize them to a uniform length for model processing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download NLTK tokenizer data (run once if not already downloaded)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Assuming sentences_train is your training data (list of strings)\n",
        "train_iter = sentences_train\n",
        "\n",
        "# Define a simple tokenizer using NLTK (or you can use another library like spacy)\n",
        "def tokenizer(text):\n",
        "    return word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "\n",
        "# Function to yield tokens from the data\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# Build vocabulary manually using Counter\n",
        "def build_vocab_from_iterator(token_iterator, specials=('<unk>', '<pad>')):\n",
        "    counter = Counter()\n",
        "    for tokens in token_iterator:\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # Create vocab dictionary with special tokens\n",
        "    vocab = {token: idx + len(specials) for idx, (token, _) in enumerate(counter.items())}\n",
        "    for idx, special in enumerate(specials):\n",
        "        vocab[special] = idx\n",
        "\n",
        "    # Reverse mapping (stoi: string to index)\n",
        "    stoi = vocab\n",
        "    # Index to string mapping\n",
        "    itos = {idx: token for token, idx in stoi.items()}\n",
        "\n",
        "    return stoi, itos, stoi['<unk>']\n",
        "\n",
        "# Build the vocabulary\n",
        "vocab, index_to_vocab, default_index = build_vocab_from_iterator(yield_tokens(train_iter), specials=('<unk>', '<pad>'))\n",
        "padding_index = vocab['<pad>']\n",
        "\n",
        "# Use CountVectorizer with the custom tokenizer and vocabulary\n",
        "vectorizer = CountVectorizer(\n",
        "    tokenizer=tokenizer,\n",
        "    vocabulary=vocab,  # Pass the string-to-index mapping\n",
        "    lowercase=True\n",
        ")\n",
        "\n",
        "# Example usage (assuming sentences_train is a list of strings)\n",
        "# vectorized_data = vectorizer.fit_transform(sentences_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOlrgrqDuGGE",
        "outputId": "20b04963-08c3-4352-8dbb-a792c851bf4c"
      },
      "id": "wOlrgrqDuGGE",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "d307b754-2034-429b-8e45-bae5ff3935b0",
      "metadata": {
        "id": "d307b754-2034-429b-8e45-bae5ff3935b0"
      },
      "outputs": [],
      "source": [
        "x_train = vectorizer.transform(sentences_train).toarray() #BOW representation\n",
        "x_dev = vectorizer.transform(sentences_dev).toarray() #BOW representation\n",
        "x_test = vectorizer.transform(sentences_test).toarray() #BOW representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "44e265d9-17c3-4a99-be69-00f018604072",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44e265d9-17c3-4a99-be69-00f018604072",
        "outputId": "bd055e2b-8353-4242-f9fe-e124022a370d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 1812)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "507118e5-2396-4177-855e-87d9b7290dd1",
      "metadata": {
        "id": "507118e5-2396-4177-855e-87d9b7290dd1"
      },
      "source": [
        "Now every sentence has been transformed into a vector with frequency count of 1814 vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "ab72774d-c736-4d25-9473-c66b8a7c04eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab72774d-c736-4d25-9473-c66b8a7c04eb",
        "outputId": "ea6dca40-f3f0-4b96-c724-4666ae6322fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size = 1812\n",
            "['wow', '...', 'loved', 'this', 'place', '.']\n",
            "[0 0 1 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "vocab_size = x_train.shape[1]\n",
        "print(\"Vocab size =\", vocab_size)\n",
        "print(tokenizer(sentences_train[0]))\n",
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e173786a-548c-4c7c-b2ed-be805b44af32",
      "metadata": {
        "id": "e173786a-548c-4c7c-b2ed-be805b44af32"
      },
      "source": [
        "## Baseline with sklearn logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3266956f-6a9a-4edf-9c4b-fdf9cdf4ae98",
      "metadata": {
        "id": "3266956f-6a9a-4edf-9c4b-fdf9cdf4ae98"
      },
      "source": [
        "Before we build a neural network model, let's see how well logistic regression do with this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "dc08fed4-29ce-45ad-a680-b559c45eba59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc08fed4-29ce-45ad-a680-b559c45eba59",
        "outputId": "97061bad-2e24-4fea-b29b-2fef641263ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.69\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(x_train, y_train)\n",
        "score = classifier.score(x_test, y_test)\n",
        "\n",
        "print(\"Accuracy:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13b9a97e-c02f-4c1c-ad52-009cb90e66f3",
      "metadata": {
        "id": "13b9a97e-c02f-4c1c-ad52-009cb90e66f3"
      },
      "source": [
        "The logistic regression result is not too bad, and it will serve as a baseline for the deep learning models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d9c647d-4b0b-428b-ba60-81968920bd41",
      "metadata": {
        "id": "4d9c647d-4b0b-428b-ba60-81968920bd41"
      },
      "source": [
        "## Short Introduction of Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b9b6ebe-7af2-490e-9ccd-0a1a634d1467",
      "metadata": {
        "id": "0b9b6ebe-7af2-490e-9ccd-0a1a634d1467"
      },
      "source": [
        "PyTorch is an open-source machine learning, especially deep learning, library widely acclaimed for its flexibility, speed, and ease of use. To learn pytorch properly please refer to their official tutorial: https://pytorch.org/tutorials/beginner/basics/intro.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49f7047a-43e7-4900-86e6-7ab786222d1b",
      "metadata": {
        "id": "49f7047a-43e7-4900-86e6-7ab786222d1b"
      },
      "source": [
        "### Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb0cab6f-18bc-4992-b7e8-42abe5e94fb8",
      "metadata": {
        "id": "bb0cab6f-18bc-4992-b7e8-42abe5e94fb8"
      },
      "source": [
        "Tensors are a specialized data structure that are very similar to numpy arrays and matrices. But more than numpy array, tensors cache and trace  the mathematic operation that the been carried; therefore capable of automatically calculate the gradient for back propagation during training. Beyond mere numerical storage, tensors uniquely track and record the mathematical operations performed on them. This intrinsic capability allows for the automatic computation of gradients, a crucial component for the backpropagation process during neural network training. When operating with your tensors, there are three important properties to look after, which are \"shape (The dimenions of the matrices)\", \"dtype (the data type of the values)\", \"device (The harware (cpu, gpu) the values are stored)\". detail please see: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "59553894-acf3-42a7-bb30-edab067899b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59553894-acf3-42a7-bb30-edab067899b4",
        "outputId": "daab590a-3caa-47b7-d3a6-56e66d2641c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "44948700-cde0-41c8-88d0-ba2d365f16b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44948700-cde0-41c8-88d0-ba2d365f16b7",
        "outputId": "0d13c906-4ea4-43c4-9edb-4f2f94bf6c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relocate your tensor to GPU memory using .cuda()"
      ],
      "metadata": {
        "id": "OuSRzXd-1Ic-"
      },
      "id": "OuSRzXd-1Ic-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    # Move tensor to GPU\n",
        "    tensor_gpu = tensor.cuda()\n",
        "\n",
        "    print(f\"Tensor is on: {tensor_gpu.device}\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABV_iGKS1TMI",
        "outputId": "64ab5903-f649-4b1a-bc47-72d2d950e130"
      },
      "id": "ABV_iGKS1TMI",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor is on: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have multiple GPU and would like to assign to a specific GPU(and even CPU, and mps)."
      ],
      "metadata": {
        "id": "P1BEyGVi1lMu"
      },
      "id": "P1BEyGVi1lMu"
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    # Move tensor to GPU (default CUDA device)\n",
        "    tensor_gpu = tensor.to('cuda:0')\n",
        "\n",
        "    # Optionally specify a specific GPU (e.g., GPU 0)\n",
        "    # tensor_gpu = tensor.to('cuda:0')\n",
        "\n",
        "    print(f\"Tensor is on: {tensor_gpu.device}\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KeIMTuN1rrC",
        "outputId": "873de0a8-9577-4714-8f2e-c39742585701"
      },
      "id": "7KeIMTuN1rrC",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor is on: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a870fe5c-ffe3-49de-ada2-65d4e2056700",
      "metadata": {
        "id": "a870fe5c-ffe3-49de-ada2-65d4e2056700"
      },
      "source": [
        "### DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64f09ff5-eeca-464d-8d84-7e8335fe8a8c",
      "metadata": {
        "id": "64f09ff5-eeca-464d-8d84-7e8335fe8a8c"
      },
      "source": [
        "For efficient training of deep learning models, it is common practice to train in batches rather than processing instances individually. Moreover, it is essential to convert the data into PyTorch tensors prior to training. Given that unstructured data, including text and images, often necessitates pre-processing, establishing a data pipeline for pre-processing becomes imperative for effective model training. This pipeline not only streamlines the preparation of data but also ensures compatibility with PyTorch's computational framework. For a detailed guide, please refer to the official tutorial: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "0c321894-a34a-424b-9ac7-6f830305b145",
      "metadata": {
        "id": "0c321894-a34a-424b-9ac7-6f830305b145"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "\n",
        "def bow_collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for  _text, _label in batch:\n",
        "        label_list.append(_label)\n",
        "        text_list.append(_text)\n",
        "    # For each batched data, we convert the values into tensor.\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
        "    text_list = torch.tensor(text_list, dtype=torch.float32)\n",
        "\n",
        "    # We also place each tensor to an assigned device\n",
        "    return text_list.to(device), label_list.reshape(-1, 1).to(device)\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(list(zip(x_train, y_train)), batch_size=batch_size, collate_fn=bow_collate_batch)\n",
        "dev_dataloader = DataLoader(list(zip(x_dev, y_dev)), batch_size=batch_size, collate_fn=bow_collate_batch)\n",
        "test_dataloader = DataLoader(list(zip(x_test, y_test)), batch_size=batch_size, collate_fn=bow_collate_batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c1ac380-1689-4e16-b450-85f9ce69291e",
      "metadata": {
        "id": "2c1ac380-1689-4e16-b450-85f9ce69291e"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fee88a7b-6f44-4246-a159-36e1bb0f3edb",
      "metadata": {
        "id": "fee88a7b-6f44-4246-a159-36e1bb0f3edb"
      },
      "source": [
        "PyTorch neural network models consist of sequential layers, each containing parameters that the network learns from during training. To create a custom model in PyTorch, your class should inherit from nn.Module, which is the base class for all neural network modules. Within the constructor method \\_\\_init\\_\\_, you can define the layers of your model. The forward propagation of the network, where the actual computation is performed, is defined in a method named forward within your class. This method specifies how data passes through the model. Below is an example of how a simple network for a Bag of Words model might be structured:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "ff0ce201-d0c2-4a91-a8b0-de3c8fdf7704",
      "metadata": {
        "id": "ff0ce201-d0c2-4a91-a8b0-de3c8fdf7704"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class BowNetwork(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.first_layer = nn.Linear(vocab_size, hidden_dim)\n",
        "        # Each layer has been randomly initaised when first created.\n",
        "        # You can further initialise the weight with different algorithms like below.\n",
        "        # torch.nn.init.uniform_(self.first_layer.weight)\n",
        "        self.second_layer = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = [batch_size, vocab_size]\n",
        "\n",
        "        x = torch.relu(self.first_layer(x))\n",
        "        # x.shape = [batch_size, hidden_size]\n",
        "\n",
        "        logits = torch.sigmoid(self.second_layer(x))\n",
        "        # logits.shape = [batch_size, 1]\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "d4bbde72-4f80-4f95-bf44-a2fb2579eed3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4bbde72-4f80-4f95-bf44-a2fb2579eed3",
        "outputId": "101fbfc9-d0e0-4f06-c679-6ee51a5a86be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BowNetwork(\n",
            "  (first_layer): Linear(in_features=1812, out_features=10, bias=True)\n",
            "  (second_layer): Linear(in_features=10, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "bow_model = BowNetwork(vocab_size, 10).to(device)\n",
        "print(bow_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f27c219b-ab41-4bfb-8282-b11e1d8d8540",
      "metadata": {
        "id": "f27c219b-ab41-4bfb-8282-b11e1d8d8540"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be0eab51-9e35-4eaa-afce-8dce8625980b",
      "metadata": {
        "id": "be0eab51-9e35-4eaa-afce-8dce8625980b"
      },
      "source": [
        "Training a neural network is an iterative process that involves updating the model's weights to optimize performance on a given task. This process unfolds over multiple iterations, known as epochs, during which the model undergoes several key steps, as outlined below:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "d76eaa9c-88aa-4e86-b35b-e77caee0eaab",
      "metadata": {
        "id": "d76eaa9c-88aa-4e86-b35b-e77caee0eaab"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    # Calling .train() will evoke the tensor to start caching steps and gradients.\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        # Compute prediction by directly calling the model variable.\n",
        "        pred = model(X)\n",
        "\n",
        "        # Calculate the loss by comparing the prediction and the true labels.\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation: calculate the gradient by walking back the cached steps.\n",
        "        loss.backward()\n",
        "        # Update the parameters of the model with the loss gradient.\n",
        "        optimizer.step()\n",
        "        # Remove all the gradient to be ready for the next training of the next batch.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch  == size - 1:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aef9681-4870-4d32-8306-0ee6271bac13",
      "metadata": {
        "id": "9aef9681-4870-4d32-8306-0ee6271bac13"
      },
      "source": [
        "It's essential to specify our optimizer—a component that dictates the learning strategy through parameters such as the learning rate—and our loss function, which measures the discrepancy between the model's predictions and the actual data. Finally, we must determine the number of epochs, or complete passes through the training dataset, to effectively guide the training process to convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "d125682a-517b-444f-b47a-3b03b545719d",
      "metadata": {
        "id": "d125682a-517b-444f-b47a-3b03b545719d"
      },
      "outputs": [],
      "source": [
        "# We select Binacy cross entropy loss\n",
        "loss_fn = nn.BCELoss()\n",
        "# We select Adam optimizer and hook it up with our model parameters.\n",
        "optimizer = torch.optim.Adam(bow_model.parameters(), lr=0.001)\n",
        "# we set epochs to 30\n",
        "epochs = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "905fe52b-7539-4090-9470-1b8f2ebdbcfa",
      "metadata": {
        "id": "905fe52b-7539-4090-9470-1b8f2ebdbcfa"
      },
      "source": [
        "During each epoch iteration, we also would like to gauge the progress of the model's performance. We can use the dev set with a test functin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "f2a21049-27d3-4c83-9ebf-9b5ba31e3737",
      "metadata": {
        "id": "f2a21049-27d3-4c83-9ebf-9b5ba31e3737"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    # After calling eval(), the model no longer caching steps and gradient.\n",
        "    # The model also does the inference faster with less resource.\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    # This line specify that there will be no gradient in the operation below.\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            result = (pred>0.5).float()\n",
        "            correct += (result == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Performance: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7c53bef-6e43-40ac-a563-fd9b97011b0e",
      "metadata": {
        "id": "a7c53bef-6e43-40ac-a563-fd9b97011b0e"
      },
      "source": [
        "Now we can run our training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "4649c55a-48ed-45b0-96cc-5e3e8f671409",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4649c55a-48ed-45b0-96cc-5e3e8f671409",
        "outputId": "c3642609-abc1-4a6a-c4a3-4aee3bbdad89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training BOW feedforward network model!\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 56.0%, Avg loss: 0.679033 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 66.0%, Avg loss: 0.632574 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 74.0%, Avg loss: 0.573498 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 79.0%, Avg loss: 0.519920 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 81.0%, Avg loss: 0.479142 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 79.0%, Avg loss: 0.449264 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 81.0%, Avg loss: 0.428555 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 81.0%, Avg loss: 0.414380 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 81.0%, Avg loss: 0.404627 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 81.0%, Avg loss: 0.398217 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 81.0%, Avg loss: 0.393792 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 82.0%, Avg loss: 0.391316 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 82.0%, Avg loss: 0.389951 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 83.0%, Avg loss: 0.389760 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 83.0%, Avg loss: 0.390550 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 84.0%, Avg loss: 0.391936 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 83.0%, Avg loss: 0.394125 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 83.0%, Avg loss: 0.396567 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 82.0%, Avg loss: 0.399862 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 82.0%, Avg loss: 0.403178 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "print(\"Training BOW feedforward network model!\")\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "    train(train_dataloader, bow_model, loss_fn, optimizer)\n",
        "    test(dev_dataloader, bow_model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89734ebc-af9a-439d-b72a-86d3953e8518",
      "metadata": {
        "id": "89734ebc-af9a-439d-b72a-86d3953e8518"
      },
      "source": [
        "Now test it with the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "56885144-b792-4be0-ba96-52421b6a22d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56885144-b792-4be0-ba96-52421b6a22d5",
        "outputId": "50343886-c5d4-4a72-9d00-cb857b5d61b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final test:\n",
            "Test Performance: \n",
            " Accuracy: 76.0%, Avg loss: 0.558826 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"final test:\" )\n",
        "test(test_dataloader, bow_model, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79c86b4a-7ecf-441a-b3ae-436591643d77",
      "metadata": {
        "id": "79c86b4a-7ecf-441a-b3ae-436591643d77"
      },
      "source": [
        "How does the performance compare to logistic regression? If you run it a few times you may find that it gives slightly different numbers, and that is due to random initialisation of the model parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3cf7f23-3cdf-4dd1-8f04-bf558e281027",
      "metadata": {
        "id": "f3cf7f23-3cdf-4dd1-8f04-bf558e281027"
      },
      "source": [
        "## Embedding cosine similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26359a39-2d6b-41c7-81cb-f446f5b8a13c",
      "metadata": {
        "id": "26359a39-2d6b-41c7-81cb-f446f5b8a13c"
      },
      "source": [
        "Even though we did not explicitly define any word embeddings in the model architecture, they are in our model: in the weights between the input and the hidden layer. The hidden layer can therefore be interpreted as a sum of word embeddings for each input document.\n",
        "\n",
        "Let's fetch the word embeddings of some words, and look at their cosine similarity, and see if they make any sense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "e3e8d52f-b989-4334-a381-8c96be080a5a",
      "metadata": {
        "id": "e3e8d52f-b989-4334-a381-8c96be080a5a"
      },
      "outputs": [],
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cos_sim(a, b):\n",
        "    return dot(a, b) / (norm(a) * norm(b))\n",
        "\n",
        "def display_embedding_similarity_examples(embeddings, vocab):\n",
        "    emb_love = embeddings[vocab[\"love\"]]  # embeddings for 'love'\n",
        "    emb_like = embeddings[vocab[\"like\"]]\n",
        "    emb_lukewarm = embeddings[vocab[\"lukewarm\"]]\n",
        "    emb_bad = embeddings[vocab[\"bad\"]]\n",
        "\n",
        "    print(\"show embedding similarity examples...\")\n",
        "    print(\"embedding vector of love:\")\n",
        "    print(emb_love)\n",
        "\n",
        "    print(\"embedding cosine similarity comparisons:\")\n",
        "    print(\"love vs. like =\", cos_sim(emb_love, emb_like))\n",
        "    print(\"love vs. lukewarm =\", cos_sim(emb_love, emb_lukewarm))\n",
        "    print(\"love vs. bad =\", cos_sim(emb_love, emb_bad))\n",
        "    print(\"lukewarm vs. bad =\", cos_sim(emb_lukewarm, emb_bad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "1c22d266-e942-4d29-9729-54551e447b05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c22d266-e942-4d29-9729-54551e447b05",
        "outputId": "f48efd25-9f7d-4353-a8fe-02cfa3241634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "show embedding of bow model\n",
            "show embedding similarity examples...\n",
            "embedding vector of love:\n",
            "[ 0.25114682  0.25714073  0.26676106 -0.26078755 -0.21964015  0.27364895\n",
            " -0.258442   -0.22225012 -0.22669387 -0.22875689]\n",
            "embedding cosine similarity comparisons:\n",
            "love vs. like = 0.98100877\n",
            "love vs. lukewarm = -0.9891863\n",
            "love vs. bad = -0.9904617\n",
            "lukewarm vs. bad = 0.9966723\n"
          ]
        }
      ],
      "source": [
        "print(\"show embedding of bow model\")\n",
        "# extract word embeddings layer\n",
        "embeddings = bow_model.first_layer.weight.T.to(\"cpu\").detach().numpy()\n",
        "display_embedding_similarity_examples(embeddings, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe4f5849-3eeb-4e56-b10e-c1fa05b2cace",
      "metadata": {
        "id": "fe4f5849-3eeb-4e56-b10e-c1fa05b2cace"
      },
      "source": [
        "Not bad. You should find that for *love* and *like*, which are both positive sentiment words, produce high cosine similarity. Similar observations for *lukewarm* and *bad*. But when we compare opposite polarity words like *love* and *bad*, we get negative cosine similarity values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3c9eab3-65ed-4210-b35a-9be2f787786a",
      "metadata": {
        "id": "e3c9eab3-65ed-4210-b35a-9be2f787786a"
      },
      "source": [
        "## Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fbd0694-70e3-41be-8692-b4d159c5f77c",
      "metadata": {
        "id": "6fbd0694-70e3-41be-8692-b4d159c5f77c"
      },
      "source": [
        "Next, we are going to build another feed-forward model, but this time, instead of using BOW features as input, we want to use the word sequence as input (so order of words is preserved). It is usually not straightforward to do this for classical machine learning models, but with neural networks and embeddings, it's pretty straightforward.\n",
        "\n",
        "Let's first build a pipeline by combining vocab and tokenizer together that can convert a sentence into a number sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0059c57-81cf-4856-825d-6fcf7e9dbe01",
      "metadata": {
        "id": "f0059c57-81cf-4856-825d-6fcf7e9dbe01"
      },
      "source": [
        "### Preparing sequence data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "5b8ccc7e-4e6d-4458-9a9c-714f4c082b8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b8ccc7e-4e6d-4458-9a9c-714f4c082b8f",
        "outputId": "dcaefe72-f05f-4029-d0b6-85c87da21e22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1457, 894, 97, 410, 9, 71, 11, 665, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "def sequence_pipeline(x):\n",
        "  tokens = tokenizer(x)\n",
        "  return [vocab.get(token, default_index) for token in tokens]\n",
        "\n",
        "sequence_pipeline(\"Hello world, today is a good day.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99407dc1-7f85-4255-b059-fc3f0060ac26",
      "metadata": {
        "id": "99407dc1-7f85-4255-b059-fc3f0060ac26"
      },
      "source": [
        "Now lets build the pytorch dataloader pipeline that 1.) Convert every text sentence into number sequence, 2.) Padding all sentences to our predefined max sentence, so the input dimension will be consistent, 3.) convert all input to appropriate tensor and place them on device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "0a766052-0e35-4b00-afe9-c6c926a6f5e0",
      "metadata": {
        "id": "0a766052-0e35-4b00-afe9-c6c926a6f5e0"
      },
      "outputs": [],
      "source": [
        "def seq_collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for  _text, _label in batch:\n",
        "        label_list.append(_label)\n",
        "        text_list.append(sequence_pipeline(_text))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
        "\n",
        "    # Pad or truncate each sequence\n",
        "    padded_sequences = []\n",
        "    for seq in text_list:\n",
        "        # Truncate if longer than max_len\n",
        "        padded_seq = seq[:max_len]\n",
        "         # Pad if shorter\n",
        "        padded_seq += [padding_index] * (max_len - len(padded_seq))\n",
        "        padded_sequences.append(torch.tensor(padded_seq))\n",
        "    text_list = torch.stack(padded_sequences)\n",
        "    # Stack all sequences into a single tensor\n",
        "    return text_list.to(device), label_list.reshape(-1, 1).to(device)\n",
        "\n",
        "# set max length\n",
        "max_len = 30\n",
        "\n",
        "# Create data loaders.\n",
        "xseq_train_dataloader = DataLoader(list(zip(sentences_train, y_train)), batch_size=10, collate_fn=seq_collate_batch)\n",
        "xseq_dev_dataloader = DataLoader(list(zip(sentences_dev, y_dev)), batch_size=10, collate_fn=seq_collate_batch)\n",
        "xseq_test_dataloader = DataLoader(list(zip(sentences_test, y_test)), batch_size=10, collate_fn=seq_collate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3032faab-1cd9-4f50-aac9-bd13b2bbafcf",
      "metadata": {
        "id": "3032faab-1cd9-4f50-aac9-bd13b2bbafcf"
      },
      "source": [
        "## Sequence Feed Forward Network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c776c8e7-fde1-4f58-9a7b-dbf108384682",
      "metadata": {
        "id": "c776c8e7-fde1-4f58-9a7b-dbf108384682"
      },
      "source": [
        "Now let's build our second model. This model first embeds each word in the input sequence into embeddings, and then concatenate the word embeddings together to represent input sequence. The ``Flatten`` function you see after the embedding layer is essentially doing the concatenation, by 'chaining' the list of word embeddings into a very long vector.\n",
        "\n",
        "If our word embeddings has a dimension 10, and our documents always have 30 words (padded), then here the concatenated word embeddings have a dimension of 10 x 30 = 300.\n",
        "\n",
        "The concatenated word embeddings undergo a linear transformation with non-linear activations (``layers.Dense(10, activation='relu')``), producing a hidden representation with a dimension of 10. It is then passed to the output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "bcae811f-7be3-407b-9cd6-4519ff479336",
      "metadata": {
        "id": "bcae811f-7be3-407b-9cd6-4519ff479336"
      },
      "outputs": [],
      "source": [
        "class SequenceFFNetwork(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx, max_len):\n",
        "        super().__init__()\n",
        "        self.embedding  = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        # torch.nn.init.uniform_(self.embedding.weight)\n",
        "        self.first_layer = nn.Linear(hidden_dim * max_len, hidden_dim)\n",
        "        # torch.nn.init.uniform_(self.first_layer.weight)\n",
        "        self.second_layer = nn.Linear(hidden_dim, 1)\n",
        "        # torch.nn.init.uniform_(self.second_layer.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        # Flattening all word vectors in to one long vector\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = torch.relu(self.first_layer(x))\n",
        "        logits = torch.sigmoid(self.second_layer(x))\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "963c6127-adc6-4c5e-adac-aefc030546ef",
      "metadata": {
        "id": "963c6127-adc6-4c5e-adac-aefc030546ef"
      },
      "source": [
        "Now let's see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "51ec16ef-da8d-4f11-a3a9-ef567fa87c30",
      "metadata": {
        "id": "51ec16ef-da8d-4f11-a3a9-ef567fa87c30"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 10\n",
        "hidden_dim = 10\n",
        "padding_index = vocab[\"<pad>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "d4675294-9f19-4a21-84c8-33e8fb724130",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4675294-9f19-4a21-84c8-33e8fb724130",
        "outputId": "04fd5ec6-b3d9-48ae-e681-174fd140f831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training seqeunce feedforward network model!\n",
            "SequenceFFNetwork(\n",
            "  (embedding): Embedding(1812, 10, padding_idx=1)\n",
            "  (first_layer): Linear(in_features=300, out_features=10, bias=True)\n",
            "  (second_layer): Linear(in_features=10, out_features=1, bias=True)\n",
            ")\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 48.0%, Avg loss: 0.697589 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 51.0%, Avg loss: 0.692550 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 51.0%, Avg loss: 0.689816 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 52.0%, Avg loss: 0.694150 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 57.0%, Avg loss: 0.709097 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 59.0%, Avg loss: 0.726845 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 57.0%, Avg loss: 0.742752 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 57.0%, Avg loss: 0.762157 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 59.0%, Avg loss: 0.781790 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 60.0%, Avg loss: 0.799937 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 60.0%, Avg loss: 0.825282 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 60.0%, Avg loss: 0.850631 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 60.0%, Avg loss: 0.879371 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 60.0%, Avg loss: 0.907165 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 61.0%, Avg loss: 0.937766 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 60.0%, Avg loss: 0.971954 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 60.0%, Avg loss: 0.999488 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 60.0%, Avg loss: 1.033583 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 60.0%, Avg loss: 1.062773 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 60.0%, Avg loss: 1.094787 \n",
            "\n",
            "Done!\n",
            "final test:\n",
            "Test Performance: \n",
            " Accuracy: 58.0%, Avg loss: 1.261536 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Training seqeunce feedforward network model!\")\n",
        "\n",
        "seq_model = SequenceFFNetwork(vocab_size, embedding_dim, hidden_dim, padding_index, max_len=max_len).to(device)\n",
        "print(seq_model)\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(seq_model.parameters(), lr=0.001)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "    train(xseq_train_dataloader, seq_model, loss_fn, optimizer)\n",
        "    test(xseq_dev_dataloader, seq_model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "print(\"final test:\")\n",
        "test(xseq_test_dataloader, seq_model, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c30ed5-3fe5-4480-8058-7ba36cd358e5",
      "metadata": {
        "id": "c4c30ed5-3fe5-4480-8058-7ba36cd358e5"
      },
      "source": [
        "You may find that the performance isn't as good as the BOW model. In general, concatenating word embeddings isn't a good way to represent word sequence.\n",
        "\n",
        "A better way is to build a recurrent model. But first, let's extract the word embeddings for the 4 words as before and look at their similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "3b565e17-fc17-491b-b4a2-fa04374c693f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b565e17-fc17-491b-b4a2-fa04374c693f",
        "outputId": "c74c7421-b8c7-484e-e759-a16f7a92d125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "show sequence FF model embeddings\n",
            "show embedding similarity examples...\n",
            "embedding vector of love:\n",
            "[ 0.03840087 -0.6711328  -1.1095555   0.32735482  0.34282312 -1.029826\n",
            "  1.2651542  -0.19842899 -0.54404205 -1.359286  ]\n",
            "embedding cosine similarity comparisons:\n",
            "love vs. like = 0.2005474\n",
            "love vs. lukewarm = 0.17755868\n",
            "love vs. bad = -0.5497594\n",
            "lukewarm vs. bad = 0.5892816\n"
          ]
        }
      ],
      "source": [
        "print(\"show sequence FF model embeddings\")\n",
        "# extract word embeddings layer\n",
        "embeddings = seq_model.embedding.weight.to(\"cpu\").detach().numpy()\n",
        "display_embedding_similarity_examples(embeddings, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc72bdd-098e-42b8-90e8-7f06e2e8835a",
      "metadata": {
        "id": "8bc72bdd-098e-42b8-90e8-7f06e2e8835a"
      },
      "source": [
        "### LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6334b104-47b2-40e0-bfc5-c7f1949b33ae",
      "metadata": {
        "id": "6334b104-47b2-40e0-bfc5-c7f1949b33ae"
      },
      "source": [
        "Now, let's try to build an LSTM model. After the embeddings layer, the LSTM layer will process the words one at a time, and compute the next state (dimension for the hidden state = 10 in this case). The output of the LSTM layer has three components \"output (the output values of all steps)\", \"hidden(the hidden state at the end of the LSTM)\", \"cell(the cell state at the end of the LSTM)\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "28e5bbc1-3eea-47ea-8061-089c8fc8a07d",
      "metadata": {
        "id": "28e5bbc1-3eea-47ea-8061-089c8fc8a07d"
      },
      "outputs": [],
      "source": [
        "class SimpleLSTMNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, padding_idx):\n",
        "        super().__init__()\n",
        "        self.embedding  = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.forward_layer = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros((1, batch_size, hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((1, batch_size, hidden_dim)).to(device)\n",
        "        hidden = (h0, c0)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch size, seq length]\n",
        "\n",
        "        embedded = self.embedding(x)\n",
        "        # embedded = [batch size, seq length, emb dim]\n",
        "\n",
        "        h0 = self.init_hidden(x.shape[0])\n",
        "\n",
        "        output, (hidden, cell) = self.lstm_layer(embedded, h0)\n",
        "        # output = [batch size, seq length, hid dim * num directions]\n",
        "        # hidden = [num layers * num directions, batch size, hid dim]\n",
        "        # cell = [num layers * num directions, batch size, hid dim]\n",
        "\n",
        "        hidden = hidden[-1, :, :]\n",
        "        # hidden = [batch size, hid dim]\n",
        "\n",
        "        logits = torch.sigmoid(self.forward_layer(hidden))\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ea78277-98a8-4bae-9eb1-fed2f89d6405",
      "metadata": {
        "id": "4ea78277-98a8-4bae-9eb1-fed2f89d6405"
      },
      "source": [
        "Let's see how it goes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "1f99d4db-e7cc-4ea7-8829-9ca493fc974c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f99d4db-e7cc-4ea7-8829-9ca493fc974c",
        "outputId": "36e1f0e0-6145-4a9d-e521-f92c804f47ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM network model!\n",
            "SimpleLSTMNetwork(\n",
            "  (embedding): Embedding(1812, 10, padding_idx=1)\n",
            "  (lstm_layer): LSTM(10, 10, batch_first=True)\n",
            "  (forward_layer): Linear(in_features=10, out_features=1, bias=True)\n",
            ")\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 44.0%, Avg loss: 0.706505 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 44.0%, Avg loss: 0.710847 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 44.0%, Avg loss: 0.713580 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 44.0%, Avg loss: 0.715103 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 44.0%, Avg loss: 0.715810 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 44.0%, Avg loss: 0.715926 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 44.0%, Avg loss: 0.715123 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 52.0%, Avg loss: 0.710390 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 54.0%, Avg loss: 0.727038 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 55.0%, Avg loss: 0.737248 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 53.0%, Avg loss: 0.771596 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 54.0%, Avg loss: 0.778417 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 59.0%, Avg loss: 0.775600 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 60.0%, Avg loss: 0.803760 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 59.0%, Avg loss: 0.774729 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 62.0%, Avg loss: 0.782953 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 62.0%, Avg loss: 0.810921 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 59.0%, Avg loss: 0.850788 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 60.0%, Avg loss: 0.871458 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Test Performance: \n",
            " Accuracy: 60.0%, Avg loss: 0.884366 \n",
            "\n",
            "Done!\n",
            "final test:\n",
            "Test Performance: \n",
            " Accuracy: 74.0%, Avg loss: 0.712742 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Training LSTM network model!\")\n",
        "\n",
        "lstm_model = SimpleLSTMNetwork(vocab_size, embedding_dim, padding_index).to(device)\n",
        "print(lstm_model)\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "    train(xseq_train_dataloader, lstm_model, loss_fn, optimizer)\n",
        "    test(xseq_dev_dataloader, lstm_model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "print(\"final test:\")\n",
        "test(xseq_test_dataloader, lstm_model, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cefc7d13-8f33-480c-adc3-c03098144bb4",
      "metadata": {
        "id": "cefc7d13-8f33-480c-adc3-c03098144bb4"
      },
      "source": [
        "You should notice that the training is quite a bit slower, and that's because now the model has to process the sequence one word at a time. But the results should be better than the sequence FFmodel!\n",
        "\n",
        "And lastly, let's extract the embeddings and look at the their similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "2e12a44b-4c24-4119-8c27-b2ae5a32b327",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e12a44b-4c24-4119-8c27-b2ae5a32b327",
        "outputId": "414e274f-a3ac-4914-f2c4-82ffdc254d48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "show LSTM model embeddings\n",
            "show embedding similarity examples...\n",
            "embedding vector of love:\n",
            "[-0.8298477   0.38135883  0.6203263  -0.2778687   1.0426271  -2.799933\n",
            " -0.1474115  -0.5253565   0.6681899  -0.3256043 ]\n",
            "embedding cosine similarity comparisons:\n",
            "love vs. like = 0.12939441\n",
            "love vs. lukewarm = 0.30936325\n",
            "love vs. bad = 0.21451172\n",
            "lukewarm vs. bad = -0.25039604\n"
          ]
        }
      ],
      "source": [
        "print(\"show LSTM model embeddings\")\n",
        "# extract word embeddings layer\n",
        "embeddings = lstm_model.embedding.weight.to(\"cpu\").detach().numpy()\n",
        "display_embedding_similarity_examples(embeddings, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3511a00-9363-413e-b486-88542371b7e8",
      "metadata": {
        "id": "f3511a00-9363-413e-b486-88542371b7e8"
      },
      "source": [
        "However, if you run the trainig a few times, you might notice that LSTM is not always better. In this particular case, the BOW approach seems to triumph over recurrent model. Why is this the case?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "f683af49-bf61-40e7-8e8d-f118e4bbec78",
      "metadata": {
        "id": "f683af49-bf61-40e7-8e8d-f118e4bbec78"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}